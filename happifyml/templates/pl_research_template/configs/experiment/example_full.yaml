# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
  - override /trainer: deepspeed # override trainer to null so it's not loaded from main config defaults...
  - override /model: sample_model
  - override /datamodule: sample_datamodule
  - override /callbacks: default
  - override /logger: multi_loggers

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

seed: 43

trainer:
  _target_: pytorch_lightning.Trainer
  min_epochs: 20
  max_epochs: 100
  weights_summary: "top"
  gradient_clip_val: 1.0
  gpus: 2
  accumulate_grad_batches: 2

datamodule:
  _target_: src.datamodules.sample_datamodule.SampleDataModule
  data_dir: ${data_dir} # data_dir is specified in config.yaml
  batch_size: 8
  num_workers: 20

model:
  _target_: src.models.sample_model.Model
  lr: 5e-05
